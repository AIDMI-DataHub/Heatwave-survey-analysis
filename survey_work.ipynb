{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**This cell reads the sheets on google sheet and gives each column a uuid,if the same column name comes in different sheet then will get the same uuid\n",
        "**"
      ],
      "metadata": {
        "id": "RJlsiGRj1hFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aOBb-cjzeXG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import uuid\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Dictionary to store UUIDs for each unique column name across sheets\n",
        "uuid_mapping = {}\n",
        "\n",
        "# Function to generate UUID for columns and apply to DataFrame\n",
        "def apply_uuid_to_columns(df):\n",
        "    new_columns = {}\n",
        "    for col in df.columns:\n",
        "        if col in uuid_mapping:\n",
        "            new_columns[col] = f\"{col}--{uuid_mapping[col]}\"\n",
        "        else:\n",
        "            generated_uuid = str(uuid.uuid4())\n",
        "            uuid_mapping[col] = generated_uuid\n",
        "            new_columns[col] = f\"{col}--{generated_uuid}\"\n",
        "\n",
        "    df.rename(columns=new_columns, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Function to upload DataFrame to a specific sheet in a new Google Sheet\n",
        "def upload_to_new_google_sheets(df, new_sheet_url, creds_json, specific_sheet_name=None):\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/drive']\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_json, scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # Open the new Google Sheet by its URL\n",
        "    new_sheet = client.open_by_url(new_sheet_url)\n",
        "\n",
        "    # Create the sheet if it doesn't exist\n",
        "    try:\n",
        "        worksheet = new_sheet.worksheet(specific_sheet_name)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        worksheet = new_sheet.add_worksheet(title=specific_sheet_name, rows=\"600\", cols=\"600\")\n",
        "\n",
        "    # Clear the sheet before uploading new data (optional)\n",
        "    worksheet.clear()\n",
        "\n",
        "    # Write DataFrame to the new Google Sheet\n",
        "    set_with_dataframe(worksheet, df)\n",
        "\n",
        "    print(f\"Data uploaded to {new_sheet_url}, sheet: {specific_sheet_name}\")\n",
        "\n",
        "# Main function to process all sheets from the input Google Sheet\n",
        "def process_and_upload_all_sheets(input_sheet_url, new_sheet_url, creds_json):\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/drive']\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(creds_json, scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # Open the input Google Sheet by its URL\n",
        "    input_sheet = client.open_by_url(input_sheet_url)\n",
        "\n",
        "    # Get all sheet names from the input Google Sheet\n",
        "    sheet_names_list = [sheet.title for sheet in input_sheet.worksheets()]\n",
        "\n",
        "    # Loop through each sheet\n",
        "    for sheet_name in sheet_names_list:\n",
        "        # Read the sheet as a DataFrame\n",
        "        worksheet = input_sheet.worksheet(sheet_name)\n",
        "        df = get_as_dataframe(worksheet, dtype=str)  # Convert all values to string to avoid issues\n",
        "\n",
        "        # Apply UUIDs to columns\n",
        "        df_with_uuid = apply_uuid_to_columns(df)\n",
        "\n",
        "        # Upload to the new Google Sheet with the same sheet name\n",
        "        upload_to_new_google_sheets(df_with_uuid, new_sheet_url, creds_json, specific_sheet_name=sheet_name)\n",
        "\n",
        "# Example usage\n",
        "creds_json = ''  # Path to your credentials JSON\n",
        "input_sheet_url = ''  # Input Google Sheet URL\n",
        "new_sheet_url = ''  # Output Google Sheet URL\n",
        "\n",
        "# Process and upload all sheets from input to new Google Sheet\n",
        "process_and_upload_all_sheets(input_sheet_url, new_sheet_url, creds_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*this cell gets all the unique responses in all the columns (if the responses are seperated by ';') and make a dataframe which contains columns like Question,Question UUID,\tResponses,\tCategory,\tProject ID.\n",
        "*"
      ],
      "metadata": {
        "id": "bs_gPuFB12hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "wb = gc.open_by_key('1lIThn4TGgm-6SyQgVN9qYJeLr7BdsgtTGeot77XFdbI')\n",
        "ws = wb.worksheet('Odisha')\n",
        "rows = ws.get_all_values()\n",
        "df = pd.DataFrame(rows[1:], columns=rows[0])\n",
        "\n",
        "# Create a list of questions and their corresponding responses for 'Action'\n",
        "questions_actions = [\n",
        "    '2.4 What is the most viable action for addressing the challenges of heatwaves, particularly for health, mental well-being, and daily routines in your community?--3e4301bf-ecd9-4785-a152-286e7fea4c6e',\n",
        "    '3.4 Which initiative should be prioritized to support poor communities in managing income loss, business operation disruptions, and employment instability?--7b079dee-2c59-40c8-9ea6-6ada85e14af7',\n",
        "    '4.4 How can poor communities best mitigate the financial burden caused by heatwaves, including the costs associated with water, cooling, and healthcare?--67512f63-05ae-4589-af9b-63c8607113e9',\n",
        "    '6.6 What specific demands are communities making for anticipatory measures to combat the risks associated with heat waves?--93fd070b-50d0-473e-8ddf-563fabba36f0',\n",
        "    '7.1 Which of the following cash-based relief interventions do you believe would be most beneficial for victims of heatwaves?--4a60da82-0983-4b0a-b392-6b93af8958ba',\n",
        "    '7.2 In the wake of a severe heatwave, what type of compensation should be offered to individuals and families affected by hospitalization, income loss, and death?--0dedcb9e-a396-4f06-89ba-742d033a74ea',\n",
        "    '7.3 What low-cost financing mechanisms are most effective for supporting heatwave mitigation and adaptation efforts in vulnerable communities?--77498aad-1b17-4dfa-bb00-223e2857feaf',\n",
        "    '7.4 What is the most appropriate way to adapt social security and protection systems to assist victims of heatwaves?--bc7a0673-0dd9-4f51-8a51-3a5e8ba73e80',\n",
        "    '7.5 Which of the following is the most pressing need for individuals affected by heatwaves?--bf16755f-d99c-4b4e-8759-a066d3895365'\n",
        "]\n",
        "\n",
        "# Get responses for each action question\n",
        "responses_actions = [\n",
        "    df.get('2.4 What is the most viable action for addressing the challenges of heatwaves, particularly for health, mental well-being, and daily routines in your community?--3e4301bf-ecd9-4785-a152-286e7fea4c6e', '').astype(str),\n",
        "    df.get('3.4 Which initiative should be prioritized to support poor communities in managing income loss, business operation disruptions, and employment instability?--7b079dee-2c59-40c8-9ea6-6ada85e14af7', '').astype(str),\n",
        "    df.get('4.4 How can poor communities best mitigate the financial burden caused by heatwaves, including the costs associated with water, cooling, and healthcare?--67512f63-05ae-4589-af9b-63c8607113e9', '').astype(str),\n",
        "    df.get('6.6 What specific demands are communities making for anticipatory measures to combat the risks associated with heat waves?--93fd070b-50d0-473e-8ddf-563fabba36f0', '').astype(str),\n",
        "    df.get('7.1 Which of the following cash-based relief interventions do you believe would be most beneficial for victims of heatwaves?--4a60da82-0983-4b0a-b392-6b93af8958ba', '').astype(str),\n",
        "    df.get('7.2 In the wake of a severe heatwave, what type of compensation should be offered to individuals and families affected by hospitalization, income loss, and death?--0dedcb9e-a396-4f06-89ba-742d033a74ea', '').astype(str),\n",
        "    df.get('7.3 What low-cost financing mechanisms are most effective for supporting heatwave mitigation and adaptation efforts in vulnerable communities?--77498aad-1b17-4dfa-bb00-223e2857feaf', '').astype(str),\n",
        "    df.get('7.4 What is the most appropriate way to adapt social security and protection systems to assist victims of heatwaves?--bc7a0673-0dd9-4f51-8a51-3a5e8ba73e80', '').astype(str),\n",
        "    df.get('7.5 Which of the following is the most pressing need for individuals affected by heatwaves?--bf16755f-d99c-4b4e-8759-a066d3895365', '').astype(str)\n",
        "]\n",
        "\n",
        "# Create a list of questions and their corresponding responses for 'Measures Anticipated'\n",
        "questions_measures_anticipated = [\n",
        "    '6.2 What behavioral adjustments do you make in response to heatwave advisories?--20023e40-ac79-4c5b-91f2-3fdd47bb0a29',\n",
        "    '6.3 How do you modify your home in response to heatwave advisories?--5798d0ef-5802-45e8-91ef-634449697b32',\n",
        "    '6.5 Which of the following approaches have you considered for financing losses and damages caused by heatwaves?--959c3f83-7892-4e8c-9385-7566a6a6681b',\n",
        "    '6.4 Since the last heatwave, what investments have you made to reduce the impact of future heatwaves on your household?--c54419b1-8d70-4afc-b23c-f6c732793b84'\n",
        "]\n",
        "\n",
        "# Get responses for each measures question\n",
        "responses_measures_anticipated = [\n",
        "    df.get('6.2 What behavioral adjustments do you make in response to heatwave advisories?--20023e40-ac79-4c5b-91f2-3fdd47bb0a29', '').astype(str),\n",
        "    df.get('6.3 How do you modify your home in response to heatwave advisories?--5798d0ef-5802-45e8-91ef-634449697b32', '').astype(str),\n",
        "    df.get('6.5 Which of the following approaches have you considered for financing losses and damages caused by heatwaves?--959c3f83-7892-4e8c-9385-7566a6a6681b', '').astype(str),\n",
        "    df.get('6.4 Since the last heatwave, what investments have you made to reduce the impact of future heatwaves on your household?--c54419b1-8d70-4afc-b23c-f6c732793b84', '').astype(str)\n",
        "]\n",
        "\n",
        "# Function to create a DataFrame from questions and responses, including project ID\n",
        "def create_df(questions, responses, category, df_main):\n",
        "    question_list = []\n",
        "    question_uuid_list = []\n",
        "    unique_responses_list = []\n",
        "\n",
        "    for i, res_series in enumerate(responses):\n",
        "        full_question = questions[i]\n",
        "        question, uuid = full_question.split('--')  # Split by '--' to separate question and UUID\n",
        "        unique_responses = res_series.drop_duplicates().tolist()  # Get unique responses\n",
        "\n",
        "        # Append to lists\n",
        "        question_list.append(question.strip())\n",
        "        question_uuid_list.append(uuid.strip())\n",
        "        unique_responses_list.append(unique_responses)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_combined = pd.DataFrame({\n",
        "        'Question': question_list,\n",
        "        'Question UUID': question_uuid_list,\n",
        "        'Responses': unique_responses_list,\n",
        "        'Category': [category] * len(question_list),\n",
        "        'Project ID': df_main['Project ID--02cc4a8c-43e9-4671-9dd7-c9602345fe49'].iloc[0]  # Project ID from the original df\n",
        "    })\n",
        "    return df_combined\n",
        "\n",
        "# Create DataFrames for both categories\n",
        "df_actions = create_df(questions_actions, responses_actions, 'Action', df)\n",
        "df_measures = create_df(questions_measures_anticipated, responses_measures_anticipated, 'Measures Anticipated', df)\n",
        "\n",
        "# Combine both DataFrames\n",
        "df_Odisha = pd.concat([df_actions, df_measures], ignore_index=True)\n",
        "\n",
        "# Display the final DataFrame\n",
        "df_Odisha\n",
        "\n",
        "\n",
        "# Function to extract unique values from a string column separated by ';'\n",
        "def get_unique_values_from_column(series):\n",
        "    all_values = []\n",
        "    for row in series:\n",
        "        if isinstance(row, str):  # Ensure it's a string before processing\n",
        "            all_values.extend([val.strip() for val in row.split(';') if val.strip()])\n",
        "    unique_values = list(set(all_values))  # Get unique responses across all rows\n",
        "    return unique_values\n",
        "\n",
        "# Function to create a DataFrame from questions and responses, including project ID\n",
        "def create_df(questions, responses, category, df_main):\n",
        "    question_list = []\n",
        "    question_uuid_list = []\n",
        "    unique_responses_list = []\n",
        "\n",
        "    for i, res_series in enumerate(responses):\n",
        "        full_question = questions[i]\n",
        "        question, uuid = full_question.split('--')  # Split by '--' to separate question and UUID\n",
        "\n",
        "        # Apply the unique value extraction function to each response column\n",
        "        unique_responses = get_unique_values_from_column(res_series)\n",
        "\n",
        "        # Append to lists\n",
        "        question_list.append(question.strip())\n",
        "        question_uuid_list.append(uuid.strip())\n",
        "        unique_responses_list.append(unique_responses)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_combined = pd.DataFrame({\n",
        "        'Question': question_list,\n",
        "        'Question UUID': question_uuid_list,\n",
        "        'Responses': unique_responses_list,\n",
        "        'Category': [category] * len(question_list),\n",
        "        'Project ID': df_main['Project ID--02cc4a8c-43e9-4671-9dd7-c9602345fe49'].iloc[0]  # Project ID from the original df\n",
        "    })\n",
        "    return df_combined\n",
        "\n",
        "\n",
        "\n",
        "# Actions\n",
        "df_actions = create_df(questions_actions, responses_actions, 'Action', df)\n",
        "\n",
        "# Measures Anticipated\n",
        "df_measures = create_df(questions_measures_anticipated, responses_measures_anticipated, 'Measures Anticipated', df)\n",
        "\n",
        "# Measures Taken\n",
        "\n",
        "\n",
        "# Combine all DataFrames\n",
        "df_Odisha = pd.concat([df_actions, df_measures], ignore_index=True)\n",
        "\n",
        "# Display the final DataFrame\n",
        "df_Odisha\n"
      ],
      "metadata": {
        "id": "yc10dEUW3iA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If we have all the surveys like this then we can concate them together**"
      ],
      "metadata": {
        "id": "sKRwKm4s6K-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "master_df=pd.concat([df_Ahmedabad_Gujarat_1,df_Delhi,df_Gujarat_2,df_Patan_Gujarat,df_Varanasi,df_Maharashtra,df_Rajasthan, df_Odisha, df_Kerala, df_Uttar_pradesh, df_Lucknow_Uttar_pradesh,df_Gujarat],ignore_index=True)\n",
        "master_df"
      ],
      "metadata": {
        "id": "DLNTrI-c6n1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This cell cleans the survey by removing new line characters and seperating the question number from the question keeping it in seperate column**"
      ],
      "metadata": {
        "id": "1LWzYGfY6o1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean and format unique responses\n",
        "# Function to clean and format unique responses, including replacing line breaks with spaces\n",
        "def clean_and_format_responses(responses):\n",
        "    cleaned_responses = set([resp.replace('\\n', ' ').strip() for resp in sum(responses.tolist(), [])])\n",
        "    return [f'\"{resp}\"' for resp in cleaned_responses if resp]\n",
        "\n",
        "# Rest of the code remains the same\n",
        "\n",
        "\n",
        "# Function to extract the number from the beginning of a question\n",
        "def extract_question_number(question):\n",
        "    # Regex to match both numbers with decimals like 2.4 and numbers with trailing period like 20.\n",
        "    match = re.match(r'^\\d+\\.\\d+|^\\d+\\.', question)\n",
        "    if match:\n",
        "        return match.group(0).strip('.')  # Return the matched number, remove trailing dot if present\n",
        "    return None\n",
        "\n",
        "def remove_question_number(question):\n",
        "    return re.sub(r'^\\d+\\.\\d+|^\\d+\\.', '', question).strip()  # Remove the number and any trailing spaces\n",
        "\n",
        "# Group by 'Question UUID' and aggregate 'Responses' and 'Project ID'\n",
        "df_grouped = master_df.groupby(['Question UUID']).agg({\n",
        "    'Question': 'first',  # Keep the first 'Question' for each UUID\n",
        "    'Category': 'first',  # Keep the first 'Category' for each UUID\n",
        "    'Project ID': lambda x: list(x.unique()),  # Collect unique 'Project ID's into a list\n",
        "    'Responses': lambda x: clean_and_format_responses(x)  # Clean, remove duplicates, and format responses\n",
        "}).reset_index()\n",
        "\n",
        "# Add the 'Question Number' and clean 'Question' columns\n",
        "df_grouped['Question Number'] = df_grouped['Question'].apply(extract_question_number)  # Extract number\n",
        "df_grouped['Question'] = df_grouped['Question'].apply(remove_question_number)  # Remove number from question text\n",
        "\n",
        "# Display the result\n",
        "df_grouped\n",
        "\n"
      ],
      "metadata": {
        "id": "ngh4lgvY7Hdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}